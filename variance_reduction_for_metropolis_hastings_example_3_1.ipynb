{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlfiy565UWA2c96Cfem9kJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anthonyhu25/Variance-Reduction-Metropolis/blob/main/variance_reduction_for_metropolis_hastings_example_3_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wUiDh_e87gqZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy import random\n",
        "from numpy import linalg\n",
        "import math\n",
        "import scipy\n",
        "import scipy.stats\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import rv_continuous, rv_discrete\n",
        "from scipy.stats._distn_infrastructure import rv_frozen\n",
        "from scipy.special import logsumexp\n",
        "import warnings\n",
        "import sys\n",
        "import statistics\n",
        "import pandas as pd\n",
        "from IPython.display import display, Math, HTML"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook, as well as the code in the other notebooks in this directory, will come from [this paper](https://arxiv.org/pdf/2203.02268)."
      ],
      "metadata": {
        "id": "JC3QFgoG8wJ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example 3.1: Simulated Data Example: Gaussian Target\n"
      ],
      "metadata": {
        "id": "TZSY7BAW9Vom"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are a couple of things I must note about the setup to this problem:\n",
        "1. The coefficient-less estimator of $F: \\mu_{n,G}(F):= \\frac{1}{n}\\sum_{i=0}^{n-1}[F(x_{i}) + \\int Œ±(x_{i}, y)(G(y) - G(x))q(y|x_{i})dy]$ needs a specified function $G(x)$ and also analytically evaluate the integral inside the estimator.\n",
        "\n",
        "To first get an estimate $G$ to approximate $F$ (which can be estimated by expectation of $F$ with respect to target distribution $\\pi(F)$), we need a Gaussian approximation of $\\pi(x)$ first. We hope that $F_{\\pi ÃÉ}$ is a good approximation of the ideal function $F ^{ ÃÉ}$, which is also an estimate of $F$. For this estimation, we set $G$\n",
        "\n",
        "To estimate the integral $‚à´ \\alpha(x_{i}, y)(G(y) - G(x))q(y|x_{i})dy$, we use Monte-Carlo estimates $Œ±(x_{i}, y_{i})(G(y_{i}) - G(x_{i})), y_{i} \\sim q(y|x_{i})$. To further reduce the variance of this estimator (since the Monte-Carlo estimates of the integral can have a high variance) we add in control variate $h(x_{i}, y)$ and $E_{q(y|x_{i})}(h(x_{i},y))$. Note that these terms $E_{q(y|x_{i})}(h(x_{i},y))$ and $h(x_{i},y)$ are static control variates, and also depends on the Gaussian approximation of $\\pi(x)$.\n",
        "\n",
        "So, to estimate the coefficient-less estimator of $F$ above, we use Monte-Carlo methods and use:\n",
        "\n",
        "$\\large \\mu_{n, G}(F) := \\frac{1}{n}\\sum_{i=0}^{n-1}[F(x_{i}) + \\alpha(x_{i}, y_{i})(G(y_{i}) - G(x_{i})) + h(x_{i}, y_{i}) - E_{q(y|x_{i})}[h(x_{i}, y)]]$\n",
        "\n",
        "2. To obtain our static control variate $h(x_{i}, y)$, we first need Gaussian approximations of our target $\\pi(x)$ and proposal $q(y|x)$ - let us name them $\\pi^{ÃÉ}(x)$ and $q^{ÃÉ}(y|x)$ respectively - and the function $G(x)$. Then, we set $h(x,y)$ to be the product of the Metropolis-Hastings acceptance ratio between $\\pi^{ÃÉ}(x)$ and $q^{ÃÉ}(y|x)$, and the difference between $G(y)$ and $G(x)$. Formally,\n",
        "\n",
        "$\\large h(x,y) = min(1, r^{ÃÉ}(x,y))[G(y)-G(x)]$\n",
        "\n",
        "where $r^{ÃÉ}(x,y) = \\frac{\\pi^{ÃÉ}(y)q^{ÃÉ}(x|y)}{\\pi^{ÃÉ}(x)q^{ÃÉ}(y|x)}$\n",
        "\n",
        "We hope that the acceptance ratio of the Gaussian approximations also approximates the true acceptance ratio between the proposal and the density distributions."
      ],
      "metadata": {
        "id": "4NyNjfUAhlGC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Back to the beginning...\n",
        "\n",
        "The paper begins by assuming a Markov transition kernel $P$ invariant to a target $\\pi$ (if the Markov Chain transition kernel is defined as $P$ and if the current state is distributed as some distribution $\\pi$, then after one step the current state is still distributed as $\\pi$), a function $G(x)$, and conditional next-step expectation of $G(x)$ with respect to transition kernel $P$ as $PG(x)$, given current state $x$.\n",
        "\n",
        "We can represent the conditional expectation $PG(x)$ as:\n",
        "\n",
        "$\\large PG(x) := \\int P(x, dy)G(y) = G(x) + \\int \\alpha(x,y)(G(y) - G(x))q(y|x)dy$\n",
        "\n",
        "where $\\alpha(x,y)  = min(1, r(x,y))$, and $r(x,y) := \\frac{\\pi(y)q(x|y)}{\\pi(x)q(y|x)}$\n",
        "\n",
        "Suppose we have $n$ correlated samples from target density $\\pi$. The estimator $\\mu_{n,G}$ is unbiased:\n",
        "\n",
        "$\\large \\mu_{n,G}(F) = \\frac{1}{n}\\sum_{i=1}^{n}[F(x_{i}) + PG(x_{i}) - G(x_{i})]$\n",
        "\n",
        "We substitute $PG(x)$ into $\\mu_{n,G}(F)$ and obtain:\n",
        "\n",
        "$\\large \\mu_{n,G}(F) = \\frac{1}{n}\\sum_{i=1}^{n}[F(x_{i}) + \\int \\alpha(x_{i},y)(G(y)-G(x))q(y|x_{i})dy]$\n",
        "\n",
        "Then, we approximate the integral $\\int \\alpha(x_{i},y)(G(y)-G(x))q(y|x_{i})dy$ using a single-sample Monte-Carlo estimate $\\alpha(x_{i},y_{i})(G(y_{i}) - G(x_{i})), y_{i} \\sim q(y|x_{i})$.\n",
        "\n",
        "Also, we seek to reduce the variance of the unbiased estimator $\\alpha(x_{i},y_{i})(G(y_{i}) - G(x_{i}))$ by adding in a static control variate terms $h(x_{i}, y_{i})$ and $ùîº_{q(y|x_{i})}[h(x_{i},y)]$, which both depends on the Gaussian approximation $\\pi^{ÃÉ}(x) = N(x|\\mu, \\Sigma)$ of the target distribution $\\pi(x)$.\n",
        "\n",
        "So, the final estimator $\\mu_{n,G}(F)$ becomes:\n",
        "\n",
        "$\\large \\mu_{n,G}(F) = \\frac{1}{n}\\sum_{i=1}^{n}[F(x_{i}) + \\alpha(x_{i},y_{i})(G(y_{i})-G(x_{i})) + h(x_{i}, y_{i}) - ùîº_{q(y|x_{i})}{h(x_{i},y)}]$"
      ],
      "metadata": {
        "id": "0R7huoF5p4_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to construct the static control variates"
      ],
      "metadata": {
        "id": "UoRNxuzU5hc4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we need to construct $h(x_{i}, y_{i})$ and $ùîº_{q(y|x_{i})}{h(x_{i},y)}$ from the Gaussian approximation of the target density $\\pi^{ÃÉ}(x) \\sim N(x|\\mu \\Sigma)$. Note that $h(x_{i}, y_{i})$ is similar to $Œ±(x_{i}, y_{i})$ in that it is the acceptance ratio between the target $œÄ^{ÃÉ}(x)$ and corresponding proposal $q^{ÃÉ}(x)$ multiplied by the difference in function G. Formally, $h(x,y) = min(1, r^{ÃÉ}(x,y))[G(y)-G(x)]$.\n",
        "\n",
        "To construct our other static control variate $ùîº_{q(y|x)}{h(x,y)}$, we note that we can reuse the construction of the original $PG(x)$ [here](https://arxiv.org/pdf/2203.02268#page=5)...\n",
        "\n",
        "$\\large ùîº_{q(y|x)}{h(x,y)} = \\int h(x,y)q(y|x)dy = \\int min(1, r^{ÃÉ}(x,y))[G(y)-G(x)]q(y|x)dy$\n",
        "\n",
        "Note that we stated that $G(x) = G_{0}(L^{-1}(x-\\mu))$, so we substitute this identity back into the above equation. In essense, this is performing the \"change of variables\" transformation when going from the target/proposal to the Gaussian approximation of the target/proposal.\n",
        "\n",
        "$\\large = \\int min(1, r^{ÃÉ}(x,y))[G_{0}(L^{-1}(y-\\mu))-G_{0}(L^{-1}(x-\\mu))]q(y|x)dy$\n",
        "\n",
        "Since $r^{ÃÉ}(x,y)$"
      ],
      "metadata": {
        "id": "DI5hrJSR5rIU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The target distribution of this example is a d-variate standard Gaussian distribution $N(0_{d}, I_{d})$ with a proposal distribution $q(y|x) ‚àº N(y|x, c^{2}I_{d})$ where $c^{2} = 2.38^{2}/d$ for the Random Metropolis Walk case. We are interested in estimating the expected value of the first coordinate of the target, so $F(x) = x^{(1)}$\n",
        "\n",
        "To begin, we need to construct our function $G(x)$ and its conditional expectation $PG(x) = ùîº_{x}(G(X_{1})) = ùîº_{x}[G(X_{1})|X_{0} = x]$. Note that we are given a $G_{0}(x)$ function, and transform it back to $G(x) = G_{0}(L^{-1}(x - \\mu))$, where $L$ is the Cholesky factor for the Gaussian approximation of the target distribution $œÄ(x)$, and $\\mu$ is the mean of the Gaussian approximation of the target $\\pi(x)$ -- we call this approximation $\\pi^{ÃÉ}(x) \\sim N(x|\\mu, \\Sigma)$.\n",
        "\n",
        "Since the target $N(0_{d}, I_{d})$ is already a standard Gaussian distribution, the $G(x)$ in this problem equals $G_{0}(x)$, which is defined as:\n",
        "\n",
        "$\\large G_{0}(x) = b_{0}(e^{b_{1}x^{(j)}} - e^{-b_{1}x^{(j)}}) * e^{-b_{2}||x||^{2}} +\n",
        "c_{0}(e^{-c_{1}(x^{(j)} - c_{2})^{2}} - e^{-c_{1}(x^{(j)} + c_{2})^{2}}) * e^{-c_{1} \\sum_{j^{`} \\neq j }(x^{(j^{`})})^{2}}$\n",
        "\n",
        "Note that $j$ is the coordinate we are trying to estimate from $F(x) = x^{(j)}$, so in this case $j$ equals 1. Also, $b_{0}, b_{1}, b_{2}, c_{0}, c_{1}, c_{2}$ are parameters used for the closed-form approximation of $\\alpha_{g}(x)$"
      ],
      "metadata": {
        "id": "OQqnjRA0-YE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# G_0_x function given in paper\n",
        "def G_0_x(dict_params, x, coordinate):\n",
        "  arg_1 = dict_params['b0'] *\\\n",
        "   (math.exp(dict_params['b1'] * x[coordinate - 1]) -\\\n",
        "    math.exp(-1 * dict_params['b1'] * x[coordinate - 1]))*\\\n",
        "    math.exp(dict_params['b2'] * (np.linalg.norm(x)** 2))\n",
        "  arg_2 = dict_params['c0'] * \\\n",
        "   (math.exp(-1 * dict_params['c1'] * (x[coordinate - 1] - dict_params['c2']**2)) -\\\n",
        "    math.exp(-1 * dict_params['c1'] * (x[coordinate - 1] - dict_params['c2'])** 2)) *\\\n",
        "    math.exp(-dict_params['c1'] * (float(np.linalg.norm(np.delete(x, coordinate))) ** 2))\n",
        "  return arg_1 + arg_2"
      ],
      "metadata": {
        "id": "c0aP4yoK9Vbr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pdf of a noncentral Chi-squared distribution $p(f)$ can be represented by modified Bessel function of the first kind $I_{v}(c)$, where $v$ is the degrees of freedom specified. Note that there is another representation of the pdf as an infinite sum of mixtures of Poisson and Gamma (or Poisson and Chi-squared)  distributions, but to analytically evaluate this integral we will use the modified Bessel function of the first kind\n",
        "\n",
        "So, $p(f)$ can be represented below as:\n",
        "\n",
        "$\\large p(f|k, Œª) = \\frac{1}{2}e^{-(x+\\lambda)/2} (\\frac{x}{Œª})^{\\frac{k}{4} - \\frac{1}{2}} I_{k/2-1}(\\sqrt{Œªx})$\n",
        "\n",
        "where $k$ is degrees of freedom, $\\lambda$ is noncentrality parameter, and $I_{k/2-1}(\\sqrt{Œªx})$ is the modified Bessel function of the first form, or:\n",
        "\n",
        "$\\large I_{b}(a) = (a/2)^{b} \\sum_{i=0}^{‚àû} \\frac{ (a^{2}/4)^{i}}{i! *Œì(b + i + 1)}$\n",
        "\n",
        "We need this for an analytically solving $a(x)$ and $a_{g}(x)$ for the expectation of the static control variate $ùîº_{q(y|x)}{h(x,y)}$.\n",
        "\n",
        "Note that $ùîº_{q(y|x)}{h(x,y)}$ can be represented (with minor abuse of notation) as:\n",
        "\n",
        "$\\large ùîº_{q(y|x)}{h(x,y)} = \\int min(1, r^{ÃÉ}(x^{ÃÉ},y^{ÃÉ}))[G_{0}(x^{ÃÉ}) - G_{0}(y^{ÃÉ})]q(y^{ÃÉ}|x^{ÃÉ})dy^{ÃÉ}$\n",
        "\n",
        "$\\large = G_{0}(x^{ÃÉ}) \\int min(1, r^{ÃÉ}(x^{ÃÉ},y^{ÃÉ}))q(y^{ÃÉ}|x^{ÃÉ})dy^{ÃÉ} - \\int min(1, r^{ÃÉ}(x^{ÃÉ},y^{ÃÉ}))G_{0}(y^{ÃÉ})q(y^{ÃÉ}|x^{ÃÉ})dy^{ÃÉ}$\n",
        "\n",
        "$\\large = G_{0}(x^{ÃÉ})Œ±(x^{ÃÉ}) - \\alpha_{g}(x^{ÃÉ})$\n",
        "\n"
      ],
      "metadata": {
        "id": "zj_3qOmW_ZA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Integral of the expected min of 1, and ....\n",
        "## Used for construction of a(x) and a_g(x)\n",
        "### x.. vector\n",
        "### d = degrees of freedom, and also in this case, dimension of MVN variables\n",
        "### c = step size parameter\n",
        "### is_MALA = boolean parameter, needed for calculation of tau\n",
        "def chi_squared_expectation(x, d, c, non_central_param, is_MALA):\n",
        "  norm_x = np.linalg.norm(x)\n",
        "  # calculate bounds of boundary\n",
        "  boundary_change = (norm_x/c) ** 2\n",
        "  if is_MALA == True:\n",
        "    tau = c/2\n",
        "  else:\n",
        "    tau = 1\n",
        "  # PDF of noncentral chi-squared in terms of modified bessel function\n",
        "  ## f is vector\n",
        "  ## d is degrees of freedom\n",
        "  ### v is non-centrality parameter\n",
        "  def pdf_chi_squared(f, d, v):\n",
        "    return 0.5 * np.exp(-(f + v)/2) * ((f/v) ** (d / 4 - 1/2)) * scipy.special.iv(d / 2 -1, math.sqrt(v * f))\n",
        "  # Calculate integral \"left\" of boundary\n",
        "  ## in terms of f, because f is the non-central chi-squared distribution\n",
        "  ### product integrated over from 0 to boundary change\n",
        "  ### Function: pdf of chi-squared multiplied by some exponential term\n",
        "  left_integral = scipy.integrate.quad(lambda f: pdf_chi_squared(f, d, non_central_param) , 0, boundary_change)\n",
        "  # Calculate integral \"rigtht\" of boundary\n",
        "  ## much simpler: only in terms of chi-squared pdf\n",
        "  right_integral = scipy.integrate.quad(lambda f: pdf_chi_squared(f, d, non_central_param) *\\\n",
        "                                       np.exp(-( (c * tau) ** 2 / 2) * (f - boundary_change)),\n",
        "                                        boundary_change, np.inf)\n",
        "  return left_integral[0] + right_integral[0]"
      ],
      "metadata": {
        "id": "y573nDDhuk63"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I do notice some mistakes in the paper. The integral of the min function with respect to the noncentral chi-square distributions appear to have the wrong bounds -- I believe they should be swapped.\n",
        "\n",
        "Also, there is inconsistent naming of the noncentral parameter for $a(x)$"
      ],
      "metadata": {
        "id": "tVsHlV0ABDZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculation of a_g_x\n",
        "## Note that this is basically a weighted mixtured of the sum of K chi-squared min expectations\n",
        "## x: vector\n",
        "## K: number of components used\n",
        "## c: stepsize, also used for calculation of r\n",
        "## d: dimension of sample/vector x\n",
        "## beta_list: list of vectors b1,...bK\n",
        "## rho_list: list of scalars rho1,...rhoK\n",
        "## delta_list: list of vectors delta1,...deltaK\n",
        "## is_MALA: boolean based on whether algorithm is MALA or RWM(true if MALA, false otherwise)\n",
        "\n",
        "\n",
        "def a_g_x_calculation(x, K, c, d, beta_list, rho_list, delta_list, is_MALA):\n",
        "  r = 1\n",
        "  if is_MALA == True:\n",
        "    r = 1 - (c ** 2)/2\n",
        "  sum = 0\n",
        "  for i in range(K):\n",
        "    # Calculate m_k(x)\n",
        "    m_k_x = (r * x + (c ** 2) * (beta_list[i] + np.multiply(rho_list[i], delta_list[i]))) / (1 + 2 * float(rho_list[i]) * (c ** 2))\n",
        "    # Calculate A_k(x)\n",
        "    A_k = ((1 + 2 * float(rho_list[i]) * (c ** 2)) ** (-d/2)) *\\\n",
        "     float(np.exp( (-1/2 * (r * np.linalg.norm(x)/c)) ** 2) -\\\n",
        "    (rho_list[i] * float(np.linalg.norm(delta_list[i]) ** 2)) +\\\n",
        "    (float(np.linalg.norm(m_k_x)) ** 2)/(2 * (c ** 2) * (1 + 2 * float(rho_list[i]) * (c ** 2))))\n",
        "    # Find the \"c\" parameter for calculating expectation\n",
        "    ## It's called s_k2 for each k = 1,...K\n",
        "    s_k2 =  float(np.sqrt( (c ** 2)/ (1 + 2 * (c ** 2) * rho_list[i])))\n",
        "    ## calculate noncentral parameter for k\n",
        "    noncentral_param_k = float(np.linalg.norm(m_k_x)/c)**2\n",
        "    sum += A_k * chi_squared_expectation(x, d, c, noncentral_param_k, is_MALA)\n",
        "  return sum"
      ],
      "metadata": {
        "id": "PR4Sr8KVKUFr"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary of parameters\n",
        "## Given in example for RWM\n",
        "\n",
        "rwm_params = dict(\n",
        "    b0=8.7078,\n",
        "    b1=0.2916,\n",
        "    b2=0.0001,\n",
        "    c0=-3.5619,\n",
        "    c1=0.1131,\n",
        "    c2=3.9162\n",
        ")\n"
      ],
      "metadata": {
        "id": "ikeQg8HGWxy1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rwm_params[\"b1\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Sv3VvCT5rLM",
        "outputId": "2734b5b4-5ce7-4a60-b524-b9e4c4a551c6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2916"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper functions to generate beta, rho, and delta parameters from the lists\n",
        "def params_to_beta_vector(params, coordinate, K):\n",
        "  beta_list = []\n",
        "  # Start with beta_1\n",
        "  beta_1 = [0 for _ in range(K)]\n",
        "  beta_1[coordinate - 1] = params['b1']\n",
        "  beta_list.append(beta_1)\n",
        "  # Beta_2 = -1 * beta_1\n",
        "  beta_2 = [-1 * beta_1[i] for i in range(K)]\n",
        "  beta_list.append(beta_2)\n",
        "  # Rest of betas all 0 vector\n",
        "  for j in range(2, K):\n",
        "    beta_list.append([0 for _ in range(K)])\n",
        "  return beta_list\n",
        "\n",
        "def params_to_delta_vector(params, coordinate, K):\n",
        "  delta_list = []\n",
        "  # First two are 0 vector\n",
        "  for j in range(K - 2):\n",
        "    delta_list.append([0 for _ in range(K)])\n",
        "  # create delta_3rd\n",
        "  delta_3 = [0 for _ in range(K)]\n",
        "  delta_3[coordinate - 1] = params['c2']\n",
        "  delta_list.append(delta_3)\n",
        "  # delta 4 = -1 * delta_3\n",
        "  delta_4 = [-1 * delta_3[i] for i in range(K)]\n",
        "  delta_list.append(delta_4)\n",
        "  return delta_list\n",
        "\n",
        "def params_to_rho_scalar(params, K):\n",
        "  rho_list = []\n",
        "  for i in range(2):\n",
        "    rho_list.append(params['b2'])\n",
        "  for j in range(2, 4):\n",
        "    rho_list.append(params['c1'])\n",
        "  return rho_list"
      ],
      "metadata": {
        "id": "AZ0OPy82yNJH"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def metropolis_hastings_example_3_1(d, params, is_MALA, n_burn_in, n_samples, T_iterations, K):\n",
        "  # Initiate mu_lists: will append each 1:T_iterations\n",
        "  mu_MC = []\n",
        "  mu_CV = []\n",
        "  mu_CV_coeff = []\n",
        "  acceptance_counter_list = []\n",
        "  target_density = scipy.stats.multivariate_normal(mean = [0 for _ in range(d)], cov = np.eye(d))\n",
        "  # Calculate c and r parameters before we initiate chain\n",
        "  ## r depends on if we are doing MALA or RWM\n",
        "  c_squared = (2.38 ** 2)/d\n",
        "  r = 1\n",
        "  for i in range(T_iterations):\n",
        "    # X Chain is state of chain\n",
        "    # Y Chain is proposal state of chain\n",
        "    alpha_chain = []\n",
        "    X_chain = []\n",
        "    Y_chain = []\n",
        "    acceptance_counter = 0\n",
        "    # Initiate proposal distribution\n",
        "    proposal_density = lambda z : scipy.stats.multivariate_normal(mean = z, cov = c_squared * np.eye(d))\n",
        "    for j in range(n_samples + n_burn_in):\n",
        "      if j == 0:\n",
        "        # Accept with probability 1\n",
        "        Y_j = proposal_density(z = [0 for _ in range(d)]).rvs()\n",
        "        alpha_j = 1\n",
        "        alpha_chain.append(alpha_j)\n",
        "        Y_chain.append(Y_j.astype(float))\n",
        "        X_chain.append(Y_j.astype(float))\n",
        "      else:\n",
        "        # Get current state\n",
        "        x = X_chain[len(X_chain)-1]\n",
        "        y = proposal_density(z = r * x).rvs()\n",
        "        alpha_j = min(1, (proposal_density(z = r * y).pdf(x) * target_density.pdf(y))/(proposal_density(z = r * x).pdf(y) * target_density.pdf(x)))\n",
        "        # Generate uniform for acceptance/rejection\n",
        "        U = scipy.stats.uniform(0, 1).rvs()\n",
        "        if U <= alpha_j:\n",
        "          # Cannot throw out burn-in samples for X and Y chains yet. need them for the others,\n",
        "          # but for alpha chains and acceptance we can start counting once burn-in period stops\n",
        "          Y_chain.append(y.astype(float))\n",
        "          X_chain.append(y.astype(float))\n",
        "          if j >= n_burn_in:\n",
        "            alpha_chain.append(float(alpha_j))\n",
        "            acceptance_counter += 1\n",
        "        else:\n",
        "          Y_chain.append(y.astype(float))\n",
        "          X_chain.append(x.astype(float))\n",
        "          if j >= n_burn_in:\n",
        "            alpha_chain.append(float(alpha_j))\n",
        "    # After chain is simulated, drop burn-in samples\n",
        "    alpha_chain = alpha_chain[:n_burn_in]\n",
        "    X_chain = X_chain[:n_burn_in]\n",
        "    Y_chain = Y_chain[:n_burn_in]\n",
        "    # Calculate means\n",
        "    ## Base MC method\n",
        "    mu_MC_i = np.mean([X_chain[j][0] for j in range(len(X_chain))])\n",
        "    mu_MC.append(float(mu_MC_i))\n",
        "    ## Control variate\n",
        "    ### need G function and approximation of Gaussian\n",
        "    ### but since both distributions (target and proposal) are Gaussian, do not need to re-simulate/redraw, instead, calculate using G function\n",
        "    ### Also note that since the target is a standard normal, no transformation (for covariance and mean/shift) is needed for target\n",
        "    #### Static control variate term\n",
        "    h_list = [alpha_chain[i] * (G_0_x(params, X_chain[i], 1) - G_0_x(params, Y_chain[i], 1)) for i in range(len(X_chain))]\n",
        "    ### expecatation of h(x,y) wrt. proposal density\n",
        "    ### Need to integrate wrt. noncentral chi squared distribution\n",
        "    expectation_h_list = [G_0_x(params, X_chain[i], 1)*\\\n",
        "                          chi_squared_expectation(X_chain[i], d, np.sqrt(c_squared), ((r * np.linalg.norm(x = X_chain[i])) ** 2)/c_squared ,is_MALA) -\\\n",
        "                          a_g_x_calculation(X_chain[i], K, np.sqrt(c_squared), d,\n",
        "                          # helper functions for the beta, rho, and\n",
        "                          beta_list = params_to_beta_vector(params, 1, K),\n",
        "                          rho_list = params_to_rho_scalar(params, K),\n",
        "                          delta_list = params_to_delta_vector(params, 1, K),\n",
        "                          is_MALA = is_MALA)]\n",
        "    mu_CV_i = np.mean([X_chain[j][0] + alpha_chain[j]*\\\n",
        "                       (G_0_x(params, Y_chain[j], 1) - G_0_x(params, X_chain[j], 1)) +\\\n",
        "                       h_list[j] +\\\n",
        "                       expectation_h_list[j] for j in range(n_samples)])\n",
        "    mu_CV.append(mu_CV_i)\n",
        "    ## Calculate Control variate with parameter estimator\n",
        "  return mu_MC, mu_CV\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cM2R1K5f6IMw"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "beta_list = params_to_beta_vector(rwm_params, 1, 4)\n",
        "rho_list = params_to_rho_scalar(rwm_params, 4)\n",
        "delta_list = params_to_delta_vector(rwm_params,1, 4)"
      ],
      "metadata": {
        "id": "iu4fDNrJFkGQ"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_run = metropolis_hastings_example_3_1(2, rwm_params, False, 1000, 1000, 10, 4)"
      ],
      "metadata": {
        "id": "EE1oUsAaNFJD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "f178f79e-4614-4ed9-8a9e-9bb32c4a8523"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "operands could not be broadcast together with shapes (2,) (4,) ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2985826693.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetropolis_hastings_example_3_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrwm_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3205032620.py\u001b[0m in \u001b[0;36mmetropolis_hastings_example_3_1\u001b[0;34m(d, params, is_MALA, n_burn_in, n_samples, T_iterations, K)\u001b[0m\n\u001b[1;32m     65\u001b[0m     expectation_h_list = [G_0_x(params, X_chain[i], 1)*\\\n\u001b[1;32m     66\u001b[0m                           \u001b[0mchi_squared_expectation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_chain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_squared\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_chain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mc_squared\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mis_MALA\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                           a_g_x_calculation(X_chain[i], K, np.sqrt(c_squared), d,\n\u001b[0m\u001b[1;32m     68\u001b[0m                           \u001b[0;31m# helper functions for the beta, rho, and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                           \u001b[0mbeta_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams_to_beta_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3392843482.py\u001b[0m in \u001b[0;36ma_g_x_calculation\u001b[0;34m(x, K, c, d, beta_list, rho_list, delta_list, is_MALA)\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Calculate m_k(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mm_k_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbeta_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrho_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrho_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Calculate A_k(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mA_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrho_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,) (4,) "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_run)"
      ],
      "metadata": {
        "id": "CZByoBX6Nuhm",
        "outputId": "23a2bbc4-a559-4a85-e920-a5c97143c028",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.013702985642139098, 0.08223527392903787, -0.14105643670711865, 0.10267414072481419, -0.027264819485885126, 0.004665231291231265, -0.15151348072617693, 0.17664528373808552, -0.05219018155152474, -0.1403824337388552]\n"
          ]
        }
      ]
    }
  ]
}