{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMMS8Vb6QfJw63RDT044X4h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anthonyhu25/Variance-Reduction-Metropolis/blob/main/variance_reduction_for_metropolis_hastings_example_3_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wUiDh_e87gqZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy import random\n",
        "from numpy import linalg\n",
        "import math\n",
        "import scipy\n",
        "import scipy.stats\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import rv_continuous, rv_discrete\n",
        "from scipy.stats._distn_infrastructure import rv_frozen\n",
        "from scipy.special import logsumexp\n",
        "import warnings\n",
        "import sys\n",
        "import statistics\n",
        "import pandas as pd\n",
        "from IPython.display import display, Math, HTML"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook, as well as the code in the other notebooks in this directory, will come from [this paper](https://arxiv.org/pdf/2203.02268)."
      ],
      "metadata": {
        "id": "JC3QFgoG8wJ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example 3.1: Simulated Data Example: Gaussian Target\n"
      ],
      "metadata": {
        "id": "TZSY7BAW9Vom"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are a couple of things I must note about the setup to this problem:\n",
        "1. The coefficient-less estimator of $F: \\mu_{n,G}(F):= \\frac{1}{n}\\sum_{i=0}^{n-1}[F(x_{i}) + \\int Œ±(x_{i}, y)(G(y) - G(x))q(y|x_{i})dy]$ needs a specified function $G(x)$ and also analytically evaluate the integral inside the estimator.\n",
        "\n",
        "To first get an estimate $G$ to approximate $F$ (which can be estimated by expectation of $F$ with respect to target distribution $\\pi(F)$), we need a Gaussian approximation of $\\pi(x)$ first. We hope that $F_{\\pi ÃÉ}$ is a good approximation of the ideal function $F ^{ ÃÉ}$, which is also an estimate of $F$. For this estimation, we set $G$\n",
        "\n",
        "To estimate the integral $‚à´ \\alpha(x_{i}, y)(G(y) - G(x))q(y|x_{i})dy$, we use Monte-Carlo estimates $Œ±(x_{i}, y_{i})(G(y_{i}) - G(x_{i})), y_{i} \\sim q(y|x_{i})$. To further reduce the variance of this estimator (since the Monte-Carlo estimates of the integral can have a high variance) we add in control variate $h(x_{i}, y)$ and $E_{q(y|x_{i})}(h(x_{i},y))$. Note that these terms $E_{q(y|x_{i})}(h(x_{i},y))$ and $h(x_{i},y)$ are static control variates, and also depends on the Gaussian approximation of $\\pi(x)$.\n",
        "\n",
        "So, to estimate the coefficient-less estimator of $F$ above, we use Monte-Carlo methods and use:\n",
        "\n",
        "$\\large \\mu_{n, G}(F) := \\frac{1}{n}\\sum_{i=0}^{n-1}[F(x_{i}) + \\alpha(x_{i}, y_{i})(G(y_{i}) - G(x_{i})) + h(x_{i}, y_{i}) - E_{q(y|x_{i})}[h(x_{i}, y)]]$\n",
        "\n",
        "2. To obtain our static control variate $h(x_{i}, y)$, we first need Gaussian approximations of our target $\\pi(x)$ and proposal $q(y|x)$ - let us name them $\\pi^{ÃÉ}(x)$ and $q^{ÃÉ}(y|x)$ respectively - and the function $G(x)$. Then, we set $h(x,y)$ to be the product of the Metropolis-Hastings acceptance ratio between $\\pi^{ÃÉ}(x)$ and $q^{ÃÉ}(y|x)$, and the difference between $G(y)$ and $G(x)$. Formally,\n",
        "\n",
        "$\\large h(x,y) = min(1, r^{ÃÉ}(x,y))[G(y)-G(x)]$\n",
        "\n",
        "where $r^{ÃÉ}(x,y) = \\frac{\\pi^{ÃÉ}(y)q^{ÃÉ}(x|y)}{\\pi^{ÃÉ}(x)q^{ÃÉ}(y|x)}$\n",
        "\n",
        "We hope that the acceptance ratio of the Gaussian approximations also approximates the true acceptance ratio between the proposal and the density distributions."
      ],
      "metadata": {
        "id": "4NyNjfUAhlGC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Back to the beginning...\n",
        "\n",
        "The paper begins by assuming a Markov transition kernel $P$ invariant to a target $\\pi$ (if the Markov Chain transition kernel is defined as $P$ and if the current state is distributed as some distribution $\\pi$, then after one step the current state is still distributed as $\\pi$), a function $G(x)$, and conditional next-step expectation of $G(x)$ with respect to transition kernel $P$ as $PG(x)$, given current state $x$.\n",
        "\n",
        "We can represent the conditional expectation $PG(x)$ as:\n",
        "\n",
        "$\\large PG(x) := \\int P(x, dy)G(y) = G(x) + \\int \\alpha(x,y)(G(y) - G(x))q(y|x)dy$\n",
        "\n",
        "where $\\alpha(x,y)  = min(1, r(x,y))$, and $r(x,y) := \\frac{\\pi(y)q(x|y)}{\\pi(x)q(y|x)}$\n",
        "\n",
        "Suppose we have $n$ correlated samples from target density $\\pi$. The estimator $\\mu_{n,G}$ is unbiased:\n",
        "\n",
        "$\\large \\mu_{n,G}(F) = \\frac{1}{n}\\sum_{i=1}^{n}[F(x_{i}) + PG(x_{i}) - G(x_{i})]$\n",
        "\n",
        "We substitute $PG(x)$ into $\\mu_{n,G}(F)$ and obtain:\n",
        "\n",
        "$\\large \\mu_{n,G}(F) = \\frac{1}{n}\\sum_{i=1}^{n}[F(x_{i}) + \\int \\alpha(x_{i},y)(G(y)-G(x))q(y|x_{i})dy]$\n",
        "\n",
        "Then, we approximate the integral $\\int \\alpha(x_{i},y)(G(y)-G(x))q(y|x_{i})dy$ using a single-sample Monte-Carlo estimate $\\alpha(x_{i},y_{i})(G(y_{i}) - G(x_{i})), y_{i} \\sim q(y|x_{i})$.\n",
        "\n",
        "Also, we seek to reduce the variance of the unbiased estimator $\\alpha(x_{i},y_{i})(G(y_{i}) - G(x_{i}))$ by adding in a static control variate terms $h(x_{i}, y_{i})$ and $ùîº_{q(y|x_{i})}[h(x_{i},y)]$, which both depends on the Gaussian approximation $\\pi^{ÃÉ}(x) = N(x|\\mu, \\Sigma)$ of the target distribution $\\pi(x)$.\n",
        "\n",
        "So, the final estimator $\\mu_{n,G}(F)$ becomes:\n",
        "\n",
        "$\\large \\mu_{n,G}(F) = \\frac{1}{n}\\sum_{i=1}^{n}[F(x_{i}) + \\alpha(x_{i},y_{i})(G(y_{i})-G(x_{i})) + h(x_{i}, y_{i}) - ùîº_{q(y|x_{i})}{h(x_{i},y)}]$"
      ],
      "metadata": {
        "id": "0R7huoF5p4_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to construct the static control variates"
      ],
      "metadata": {
        "id": "UoRNxuzU5hc4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we need to construct $h(x_{i}, y_{i})$ and $ùîº_{q(y|x_{i})}{h(x_{i},y)}$ from the Gaussian approximation of the target density $\\pi^{ÃÉ}(x) \\sim N(x|\\mu \\Sigma)$. Note that $h(x_{i}, y_{i})$ is similar to $Œ±(x_{i}, y_{i})$ in that it is the acceptance ratio between the target $œÄ^{ÃÉ}(x)$ and corresponding proposal $q^{ÃÉ}(x)$ multiplied by the difference in function G. Formally, $h(x,y) = min(1, r^{ÃÉ}(x,y))[G(y)-G(x)]$.\n",
        "\n",
        "To construct our other static control variate $ùîº_{q(y|x)}{h(x,y)}$, we note that we can reuse the construction of the original $PG(x)$ [here](https://arxiv.org/pdf/2203.02268#page=5)...\n",
        "\n",
        "$\\large ùîº_{q(y|x)}{h(x,y)} = \\int h(x,y)q(y|x)dy = \\int min(1, r^{ÃÉ}(x,y))[G(y)-G(x)]q(y|x)dy$\n",
        "\n",
        "Note that we stated that $G(x) = G_{0}(L^{-1}(x-\\mu))$, so we substitute this identity back into the above equation. In essense, this is performing the \"change of variables\" transformation when going from the target/proposal to the Gaussian approximation of the target/proposal.\n",
        "\n",
        "$\\large = \\int min(1, r^{ÃÉ}(x,y))[G_{0}(L^{-1}(y-\\mu))-G_{0}(L^{-1}(x-\\mu))]q(y|x)dy$\n",
        "\n",
        "Since $r^{ÃÉ}(x,y)$"
      ],
      "metadata": {
        "id": "DI5hrJSR5rIU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The target distribution of this example is a d-variate standard Gaussian distribution $N(0_{d}, I_{d})$ with a proposal distribution $q(y|x) ‚àº N(y|x, c^{2}I_{d})$ where $c^{2} = 2.38^{2}/d$ for the Random Metropolis Walk case. We are interested in estimating the expected value of the first coordinate of the target, so $F(x) = x^{(1)}$\n",
        "\n",
        "To begin, we need to construct our function $G(x)$ and its conditional expectation $PG(x) = ùîº_{x}(G(X_{1})) = ùîº_{x}[G(X_{1})|X_{0} = x]$. Note that we are given a $G_{0}(x)$ function, and transform it back to $G(x) = G_{0}(L^{-1}(x - \\mu))$, where $L$ is the Cholesky factor for the Gaussian approximation of the target distribution $œÄ(x)$, and $\\mu$ is the mean of the Gaussian approximation of the target $\\pi(x)$ -- we call this approximation $\\pi^{ÃÉ}(x) \\sim N(x|\\mu, \\Sigma)$.\n",
        "\n",
        "Since the target $N(0_{d}, I_{d})$ is already a standard Gaussian distribution, the $G(x)$ in this problem equals $G_{0}(x)$, which is defined as:\n",
        "\n",
        "$\\large G_{0}(x) = b_{0}(e^{b_{1}x^{(j)}} - e^{-b_{1}x^{(j)}}) * e^{-b_{2}||x||^{2}} +\n",
        "c_{0}(e^{-c_{1}(x^{(j)} - c_{2})^{2}} - e^{-c_{1}(x^{(j)} + c_{2})^{2}}) * e^{-c_{1} \\sum_{j^{`} \\neq j }(x^{(j^{`})})^{2}}$\n",
        "\n",
        "Note that $j$ is the coordinate we are trying to estimate from $F(x) = x^{(j)}$, so in this case $j$ equals 1. Also, $b_{0}, b_{1}, b_{2}, c_{0}, c_{1}, c_{2}$ are parameters used for the closed-form approximation of $\\alpha_{g}(x)$"
      ],
      "metadata": {
        "id": "OQqnjRA0-YE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# G_0_x function given in paper\n",
        "def G_0_x(dict_params, x, coordinate):\n",
        "  arg_1 = dict_params['b0'] *\\\n",
        "   (math.exp(dict_params['b1'] * x[coordinate - 1]) -\\\n",
        "    math.exp(-1 * dict_params['b1'] * x[coordinate - 1]))*\\\n",
        "    math.exp(dict_params['b2'] * (np.linalg.norm(x)** 2))\n",
        "  arg_2 = dict_params['c0'] * \\\n",
        "   (math.exp(-1 * dict_params['c1'] * (x[coordinate - 1] - dict_params['c2']**2)) -\\\n",
        "    math.exp(-1 * dict_params['c1'] * (x[coordinate - 1] - dict_params['c2'])** 2)) *\\\n",
        "    math.exp(-dict_params['c1'] * (float(np.linalg.norm(np.delete(x, coordinate))) ** 2))\n",
        "  return arg_1 + arg_2"
      ],
      "metadata": {
        "id": "c0aP4yoK9Vbr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pdf of a noncentral Chi-squared distribution $p(f)$ can be represented by modified Bessel function of the first kind $I_{v}(c)$, where $v$ is the degrees of freedom specified. Note that there is another representation of the pdf as an infinite sum of mixtures of Poisson and Gamma (or Poisson and Chi-squared)  distributions, but to analytically evaluate this integral we will use the modified Bessel function of the first kind\n",
        "\n",
        "So, $p(f)$ can be represented below as:\n",
        "\n",
        "$\\large p(f|k, Œª) = \\frac{1}{2}e^{-(x+\\lambda)/2} (\\frac{x}{Œª})^{\\frac{k}{4} - \\frac{1}{2}} I_{k/2-1}(\\sqrt{Œªx})$\n",
        "\n",
        "where $k$ is degrees of freedom, $\\lambda$ is noncentrality parameter, and $I_{k/2-1}(\\sqrt{Œªx})$ is the modified Bessel function of the first form, or:\n",
        "\n",
        "$\\large I_{b}(a) = (a/2)^{b} \\sum_{i=0}^{‚àû} \\frac{ (a^{2}/4)^{i}}{i! *Œì(b + i + 1)}$\n",
        "\n",
        "We need this for an analytically solving $a(x)$ and $a_{g}(x)$ for the\n",
        "\n"
      ],
      "metadata": {
        "id": "zj_3qOmW_ZA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Integral of the expected min of 1, and ....\n",
        "## Used for construction of a(x) and a_g(x)\n",
        "### x.. vector\n",
        "### d = degrees of freedom, and also in this case, dimension of MVN variables\n",
        "### c = step size parameter\n",
        "### is_MALA = boolean parameter, needed for calculation of tau\n",
        "def chi_squared_expectation(x, d, c, non_central_param, is_MALA):\n",
        "  norm_x = np.linalg.norm(x)\n",
        "  # calculate bounds of boundary\n",
        "  boundary_change = (norm_x/c) ** 2\n",
        "  if is_MALA == True:\n",
        "    tau = c/2\n",
        "  else:\n",
        "    tau = 1\n",
        "  # PDF of noncentral chi-squared in terms of modified bessel function\n",
        "  ## f is vector\n",
        "  ## d is degrees of freedom\n",
        "  ### v is non-centrality parameter\n",
        "  def pdf_chi_squared(f, d, v):\n",
        "    return 0.5 * np.exp(-(f + v)/2) * ((f/v) ** (d / 4 - 1/2)) * scipy.special.iv(d / 2 -1, math.sqrt(v * f))\n",
        "  # Calculate integral \"left\" of boundary\n",
        "  ## in terms of f, because f is the non-central chi-squared distribution\n",
        "  ### product integrated over from 0 to boundary change\n",
        "  ### Function: pdf of chi-squared multiplied by some exponential term\n",
        "  left_integral = scipy.integrate.quad(lambda f: pdf_chi_squared(f, d, non_central_param) *\\\n",
        "                                       np.exp(-( (c * tau) ** 2 / 2) * (f - boundary_change)), 0, boundary_change)\n",
        "  # Calculate integral \"rigtht\" of boundary\n",
        "  ## much simpler: only in terms of chi-squared pdf\n",
        "  right_integral = scipy.integrate.quad(lambda f: pdf_chi_squared(f, d, non_central_param),\n",
        "                                        boundary_change, np.inf)\n",
        "  return left_integral[0] + right_integral[0]"
      ],
      "metadata": {
        "id": "y573nDDhuk63"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I do notice some mistakes in the paper. The integral of the min function with respect to the noncentral chi-square distributions appear to have the wrong bounds -- I believe they should be swapped.\n",
        "\n",
        "Also, there is inconsistent naming of the noncentral parameter for $a(x)$"
      ],
      "metadata": {
        "id": "tVsHlV0ABDZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculation of a_g_x\n",
        "## Note that this is basically a weighted mixtured of the sum of K chi-squared min expectations\n",
        "## x: vector\n",
        "## K: number of components used\n",
        "## c: stepsize, also used for calculation of r\n",
        "## d: dimension of sample/vector x\n",
        "## beta_list: list of vectors b1,...bK\n",
        "## rho_list: list of scalars rho1,...rhoK\n",
        "## delta_list: list of vectors delta1,...deltaK\n",
        "## is_MALA: boolean based on whether algorithm is MALA or RWM(true if MALA, false otherwise)\n",
        "\n",
        "\n",
        "def a_g_x_calculation(x, K, c, d, beta_list, rho_list, delta_list, is_MALA):\n",
        "  r = 1\n",
        "  if is_MALA == True:\n",
        "    r = 1 - (c ** 2)/2\n",
        "  sum = 0\n",
        "  for i in range(K):\n",
        "    # Calculate m_k(x)\n",
        "    m_k_x = (r * x + (c ** 2) * (beta_list[i] + rho_list[i] * delta_list[i])) / (1 + 2 * rho_list[i] * (c ** 2))\n",
        "    # Calculate A_k(x)\n",
        "    A_k = ((1 + 2 * rho_list[i] * (c ** 2)) ** (-d/2)) *\\\n",
        "     np.exp( (-1/2 * (r * np.linalg.norm(x)/c) ** 2) -\\\n",
        "    (rho_list[i] * np.linalg.norm(delta_list[i]) ** 2) +\\\n",
        "    (np.linalg.norm(m_k_x) ** 2)/(2 * (c ** 2) * (1 + 2 * rho_list[i] * (c ** 2))))\n",
        "    # Find the \"c\" parameter for calculating expectation\n",
        "    ## It's called s_k2 for each k = 1,...K\n",
        "    s_k2 =  np.sqrt( (c ** 2)/ (1 + 2 * (c ** 2) * rho_list[i]))\n",
        "    ## calculate noncentral parameter for k\n",
        "    noncentral_param_k = (np.linalg.norm(m_k_x)/c)**2\n",
        "    sum += A_k * chi_squared_expectation(x, d, c, noncentral_param_k, is_MALA)\n",
        "  return sum"
      ],
      "metadata": {
        "id": "PR4Sr8KVKUFr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary of parameters\n",
        "## Given in example for RWM\n",
        "\n",
        "rwm_params = dict(\n",
        "    b0=8.7078,\n",
        "    b1=0.2916,\n",
        "    b2=0.0001,\n",
        "    c0=-3.5619,\n",
        "    c1=0.1131,\n",
        "    c2=3.9162\n",
        ")\n"
      ],
      "metadata": {
        "id": "ikeQg8HGWxy1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def metropolis_hastings_example_3_1(d, params, is_MALA, n_burn_in, n_samples, T_iterations):\n",
        "  # Initiate mu_lists: will append each 1:T_iterations\n",
        "  mu_MC = []\n",
        "  mu_CV = []\n",
        "  mu_CV_coeff = []\n",
        "  acceptance_counter_list = []\n",
        "  target_density = scipy.stats.multivariate_normal(mean = [0 for _ in range(d)], cov = np.eye(d))\n",
        "  # Calculate c and r parameters before we initiate chain\n",
        "  ## r depends on if we are doing MALA or RWM\n",
        "  c_squared = (2.38 ** 2)/d\n",
        "  r = 1\n",
        "  for i in range(T_iterations):\n",
        "    # X Chain is state of chain\n",
        "    # Y Chain is proposal state of chain\n",
        "    alpha_chain = []\n",
        "    X_chain = []\n",
        "    Y_chain = []\n",
        "    acceptance_counter = 0\n",
        "    # Initiate proposal distribution\n",
        "    proposal_density = lambda z : scipy.stats.multivariate_normal(mean = z, cov = c_squared * np.eye(d))\n",
        "    for j in range(n_samples + n_burn_in):\n",
        "      if j == 0:\n",
        "        # Accept with probability 1\n",
        "        Y_j = proposal_density(z = [0 for _ in range(d)]).rvs()\n",
        "        alpha_j = 1\n",
        "        alpha_chain.append(alpha_j)\n",
        "        Y_chain.append(Y_j.astype(float))\n",
        "        X_chain.append(Y_j.astype(float))\n",
        "      else:\n",
        "        # Get current state\n",
        "        x = X_chain[len(X_chain)-1]\n",
        "        y = proposal_density(z = r * x).rvs()\n",
        "        alpha_j = min(1, (proposal_density(z = r * y).pdf(x) * target_density.pdf(y))/(proposal_density(z = r * x).pdf(y) * target_density.pdf(x)))\n",
        "        # Generate uniform for acceptance/rejection\n",
        "        U = scipy.stats.uniform(0, 1).rvs()\n",
        "        if U <= alpha_j:\n",
        "          # Cannot throw out burn-in samples for X and Y chains yet. need them for the others,\n",
        "          # but for alpha chains and acceptance we can start counting once burn-in period stops\n",
        "          Y_chain.append(y.astype(float))\n",
        "          X_chain.append(y.astype(float))\n",
        "          if j >= n_burn_in:\n",
        "            alpha_chain.append(float(alpha_j))\n",
        "            acceptance_counter += 1\n",
        "        else:\n",
        "          Y_chain.append(y.astype(float))\n",
        "          X_chain.append(x.astype(float))\n",
        "          if j >= n_burn_in:\n",
        "            alpha_chain.append(float(alpha_j))\n",
        "    # After chain is simulated, drop burn-in samples\n",
        "    alpha_chain = alpha_chain[:n_burn_in]\n",
        "    X_chain = X_chain[:n_burn_in]\n",
        "    Y_chain = Y_chain[:n_burn_in]\n",
        "    # Calculate means\n",
        "    ## Base MC method\n",
        "    mu_MC_i = np.mean([X_chain[j][0] for j in range(len(X_chain))])\n",
        "    mu_MC.append(float(mu_MC_i))\n",
        "    ## Control variate\n",
        "    ### need G function and approximation of Gaussian\n",
        "    ### but since both distributions (target and proposal) are Gaussian, do not need to re-simulate/redraw, instead, calculate using G function\n",
        "    ### Also note that since the target is a standard normal, no transformation (for covariance and mean/shift) is needed for target\n",
        "    #### Static control variate term\n",
        "    h_list = [alpha_chain[i] * (G_0_x(params, X_chain[i], 1) - G_0_x(params, Y_chain[i], 1)) for i in range(len(X_chain))]\n",
        "    #### expecatation of h(x,y) wrt. proposal density\n",
        "    expectation_h_list = []\n",
        "\n",
        "  return mu_MC\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cM2R1K5f6IMw"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_run = metropolis_hastings_example_3_1(2, rwm_params, False, 1000, 1000, 10)"
      ],
      "metadata": {
        "id": "EE1oUsAaNFJD"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_run)"
      ],
      "metadata": {
        "id": "CZByoBX6Nuhm",
        "outputId": "440703c7-4565-4aff-ea5c-cff0d50ff770",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.17081496481686526, -0.20454703209667127, 0.16439307839653378, -0.09199691854861056, 0.05884479445594152, -0.10575516800035115, 0.021664328514220045, 0.002757346494075698, 0.0316617068796241, 0.1699133133081956]\n"
          ]
        }
      ]
    }
  ]
}