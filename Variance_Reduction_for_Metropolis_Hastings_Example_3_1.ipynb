{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOrWdie0hbF3UykeTTpLLXw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anthonyhu25/Variance-Reduction-Metropolis/blob/main/Variance_Reduction_for_Metropolis_Hastings_Example_3_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUiDh_e87gqZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy import random\n",
        "from numpy import linalg\n",
        "import math\n",
        "import scipy\n",
        "import scipy.stats\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import rv_continuous, rv_discrete\n",
        "from scipy.stats._distn_infrastructure import rv_frozen\n",
        "from scipy.special import logsumexp\n",
        "import warnings\n",
        "import sys\n",
        "import statistics\n",
        "import pandas as pd\n",
        "from IPython.display import display, Math, HTML"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook, as well as the code in the other notebooks in this directory, will come from [this paper](https://arxiv.org/pdf/2203.02268)."
      ],
      "metadata": {
        "id": "JC3QFgoG8wJ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example 3.1: Simulated Data Example: Gaussian Target\n"
      ],
      "metadata": {
        "id": "TZSY7BAW9Vom"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are a couple of things I must note about the setup to this problem:\n",
        "1. The coefficient-less estimator of $F: \\mu_{n,G}(F):= \\frac{1}{n}\\sum_{i=0}^{n-1}[F(x_{i}) + \\int Œ±(x_{i}, y)(G(y) - G(x))q(y|x_{i})dy]$ needs a specified function $G(x)$ and also analytically evaluate the integral inside the estimator.\n",
        "\n",
        "To first get an estimate $G$ to approximate $F$ (which can be estimated by expectation of $F$ with respect to target distribution $\\pi(F)$), we need a Gaussian approximation of $\\pi(x)$ first. We hope that $F_{\\pi ÃÉ}$ is a good approximation of the ideal function $F ^{ ÃÉ}$, which is also an estimate of $F$. For this estimation, we set $G$\n",
        "\n",
        "To estimate the integral $‚à´ \\alpha(x_{i}, y)(G(y) - G(x))q(y|x_{i})dy$, we use Monte-Carlo estimates $Œ±(x_{i}, y_{i})(G(y_{i}) - G(x_{i})), y_{i} \\sim q(y|x_{i})$. To further reduce the variance of this estimator (since the Monte-Carlo estimates of the integral can have a high variance) we add in control variate $h(x_{i}, y)$ and $E_{q(y|x_{i})}(h(x_{i},y))$. Note that these terms $E_{q(y|x_{i})}(h(x_{i},y))$ and $h(x_{i},y)$ are static control variates, and also depends on the Gaussian approximation of $\\pi(x)$.\n",
        "\n",
        "So, to estimate the coefficient-less estimator of $F$ above, we use Monte-Carlo methods and use:\n",
        "\n",
        "$\\mu_{n, G}(F) := \\frac{1}{n}\\sum_{i=0}^{n-1}[F(x_{i}) + \\alpha(x_{i}, y_{i})(G(y_{i}) - G(x_{i})) + h(x_{i}, y_{i}) - E_{q(y|x_{i})}[h(x_{i}, y)]]$\n",
        "\n",
        "2. To obtain our static control variate $h(x_{i}, y)$, we first need Gaussian approximations of our target $\\pi(x)$ and proposal $q(y|x)$ - let us name them $\\pi^{ÃÉ}(x)$ and $q^{ÃÉ}(y|x)$ respectively - and the function $G(x)$. Then, we set $h(x,y)$ to be the product of the Metropolis-Hastings acceptance ratio between $\\pi^{ÃÉ}(x)$ and $q^{ÃÉ}(y|x)$, and the difference between $G(y)$ and $G(x)$. Formally,\n",
        "\n",
        "$h(x,y) = min(1, r^{ÃÉ}(x,y))[G(y)-G(x)]$\n",
        "\n",
        "where $r^{ÃÉ}(x,y) = \\frac{\\pi^{ÃÉ}(y)q^{ÃÉ}(x|y)}{\\pi^{ÃÉ}(x)q^{ÃÉ}(y|x)}$\n",
        "\n",
        "We hope that the acceptance ratio of the Gaussian approximations also approximates the true acceptance ratio between the proposal and the density distributions."
      ],
      "metadata": {
        "id": "4NyNjfUAhlGC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Back to the beginning...\n",
        "\n",
        "The paper begins by assuming a Markov transition kernel $P$ invariant to a target $\\pi$ (if the Markov Chain transition kernel is defined as $P$ and if the current state is distributed as some distribution $\\pi$, then after one step the current state is still distributed as $\\pi$), a function $G(x)$, and conditional next-step expectation of $G(x)$ with respect to transition kernel $P$ as $PG(x)$, given current state $x$.\n",
        "\n",
        "We can represent the conditional expectation $PG(x)$ as:\n",
        "\n",
        "$PG(x) := \\int P(x, dy)G(y) = G(x) + \\int \\alpha(x,y)(G(y) - G(x))q(y|x)dy$\n",
        "\n",
        "where $\\alpha(x,y)  = min(1, r(x,y))$, and $r(x,y) := \\frac{\\pi(y)q(x|y)}{\\pi(x)q(y|x)}$\n",
        "\n",
        "Suppose we have $n$ correlated samples from target density $\\pi$. The estimator $\\mu_{n,G}$ is unbiased:\n",
        "\n",
        "$\\mu_{n,G}(F) = \\frac{1}{n}\\sum_{i=1}^{n}[F(x_{i}) + PG(x_{i}) - G(x_{i})]$\n",
        "\n",
        "We substitute $PG(x)$ into $\\mu_{n,G}(F)$ and obtain:\n",
        "\n",
        "$\\mu_{n,G}(F) = \\frac{1}{n}\\sum_{i=1}^{n}[F(x_{i}) + \\int \\alpha(x_{i},y)(G(y)-G(x))q(y|x_{i})dy]$\n",
        "\n",
        "Then, we approximate the integral $\\int \\alpha(x_{i},y)(G(y)-G(x))q(y|x_{i})dy$ using a single-sample Monte-Carlo estimate $\\alpha(x_{i},y_{i})(G(y_{i}) - G(x_{i})), y_{i} \\sim q(y|x_{i})$.\n",
        "\n",
        "Also, we seek to reduce the variance of the unbiased estimator $\\alpha(x_{i},y_{i})(G(y_{i}) - G(x_{i}))$ by adding in a static control variate terms $h(x_{i}, y_{i})$ and $ùîº_{q(y|x_{i})}[h(x_{i},y)]$, which both depends on the Gaussian approximation $\\pi^{ÃÉ}(x) = N(x|\\mu, \\Sigma)$ of the target distribution $\\pi(x)$.\n",
        "\n",
        "So, the final estimator $\\mu_{n,G}(F)$ becomes:\n",
        "\n",
        "$\\mu_{n,G}(F) = \\frac{1}{n}\\sum_{i=1}^{n}[F(x_{i}) + \\alpha(x_{i},y_{i})(G(y_{i})-G(x_{i})) + h(x_{i}, y_{i}) - ùîº_{q(y|x_{i})}{h(x_{i},y)}]$"
      ],
      "metadata": {
        "id": "0R7huoF5p4_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to construct the static control variates"
      ],
      "metadata": {
        "id": "UoRNxuzU5hc4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we need to construct $h(x_{i}, y_{i})$ and $ùîº_{q(y|x_{i})}{h(x_{i},y)}$ from the Gaussian approximation of the target density $\\pi^{ÃÉ}(x) \\sim N(x|\\mu \\Sigma)$. Note that $h(x_{i}, y_{i})$ is similar to $Œ±(x_{i}, y_{i})$ in that it is the acceptance ratio between the target $œÄ^{ÃÉ}(x)$ and corresponding proposal $q^{ÃÉ}(x)$ multiplied by the difference in function G. Formally, $h(x,y) = min(1, r^{ÃÉ}(x,y))[G(y)-G(x)]$.\n",
        "\n",
        "To construct our other static control variate $ùîº_{q(y|x)}{h(x,y)}$, we note that we can reuse the construction of the original $PG(x)$ [here](https://arxiv.org/pdf/2203.02268#page=5)...\n",
        "\n",
        "$ùîº_{q(y|x)}{h(x,y)} = \\int h(x,y)q(y|x)dy = \\int min(1, r^{ÃÉ}(x,y))[G(y)-G(x)]q(y|x)dy$\n",
        "\n",
        "Note that we stated that $G(x) = G_{0}(L^{-1}(x-\\mu))$, so we substitute this identity back into the above equation. In essense, this is performing the \"change of variables\" transformation when going from the target/proposal to the Gaussian approximation of the target/proposal.\n",
        "\n",
        "$ = \\int min(1, r^{ÃÉ}(x,y))[G_{0}(L^{-1}(y-\\mu))-G_{0}(L^{-1}(x-\\mu))]q(y|x)dy$"
      ],
      "metadata": {
        "id": "DI5hrJSR5rIU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The target distribution of this example is a d-variate standard Gaussian distribution $N(0_{d}, I_{d})$ with a proposal distribution $q(y|x) ‚àº N(y|x, c^{2}I_{d})$ where $c^{2} = 2.38^{2}/d$ for the Random Metropolis Walk case. We are interested in estimating the expected value of the first coordinate of the target, so $F(x) = x^{(1)}$\n",
        "\n",
        "To begin, we need to construct our function $G(x)$ and its conditional expectation $PG(x) = ùîº_{x}(G(X_{1})) = ùîº_{x}[G(X_{1})|X_{0} = x]$. Note that we are given a $G_{0}(x)$ function, and transform it back to $G(x) = G_{0}(L^{-1}(x - \\mu))$, where $L$ is the Cholesky factor for the Gaussian approximation of the target distribution $œÄ(x)$, and $\\mu$ is the mean of the Gaussian approximation of the target $\\pi(x)$ -- we call this approximation $\\pi^{ÃÉ}(x) \\sim N(x|\\mu, \\Sigma)$.\n",
        "\n",
        "Since the target $N(0_{d}, I_{d})$ is already a standard Gaussian distribution, the $G(x)$ in this problem equals $G_{0}(x)$, which is defined as:\n",
        "\n",
        "$G_{0}(x) = b_{0}(e^{b_{1}x^{(j)}} - e^{-b_{1}x^{(j)}}) * e^{-b_{2}||x||^{2}} +\n",
        "c_{0}(e^{-c_{1}(x^{(j)} - c_{2})^{2}} - e^{-c_{1}(x^{(j)} + c_{2})^{2}}) * e^{-c_{1} \\sum_{j^{`} \\neq j }(x^{(j^{`})})^{2}}$\n",
        "\n",
        "Note that $j$ is the coordinate we are trying to estimate from $F(x) = x^{(j)}$, so in this case $j$ equals 1. Also, $b_{0}, b_{1}, b_{2}, c_{0}, c_{1}, c_{2}$ are parameters used for the closed-form approximation of $\\alpha_{g}(x)$"
      ],
      "metadata": {
        "id": "OQqnjRA0-YE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def G_0_x(dict_params, x):\n",
        "\n"
      ],
      "metadata": {
        "id": "c0aP4yoK9Vbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary of parameters\n",
        "## Given in example for RWM\n",
        "\n",
        "rwm_params = dict(\n",
        "    b0=8.7078,\n",
        "    b1=0.2916,\n",
        "    b2=0.0001,\n",
        "    c0=-3.5619,\n",
        "    c1=0.1131,\n",
        "    c2=3.9162\n",
        ")\n"
      ],
      "metadata": {
        "id": "ikeQg8HGWxy1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}